%!TEX root = skripsi.tex
%-----------------------------------------------------------------------------%
\chapter{\babEnam}
%-----------------------------------------------------------------------------%

%-----------------------------------------------------------------------------%
\section{Kesimpulan}
%-----------------------------------------------------------------------------%

Terdapat berbagai cara untuk membangun korpus \textit{Textual Entailment}, salah satu cara yang cukup efisien adalah dengan pendekatan \textit{semi-supervised learning}. Keunggulan pendekatan \textit{semi-supervised learning} adalah kemampuannya dalam mengurangi usaha manual manusia. Co-training merupakan salah satu contoh metode \textit{semi-supervised learning}. Metode Co-training pernah dicoba untuk  memperbesar ukuran korpus \textit{Textual Entailment} bahasa Inggris, dengan menjadikan isi korpus semula menjadi bibit dalam Co-training, kemudian Co-training akan memperbanyak isi korpus dengan melabeli data tidak berlabel yang dimasukkan. 

Penelitian ini berusaha mencoba menggunakan metode Co-training untuk membangun dari awal korpus \textit{Textual Entailment} Bahasa Indonesia. Untuk mengaplikasikan metode tersebut, dibutuhkan data dengan dua \textit{view} yang saling lepas. Wikipedia \textit{revision history} Bahasa Indonesia, yaitu data riwayat revisi dari artikel Wikipedia dalam Bahasa Indonesia, merupakan salah satu sumber data yang memiliki kriteria tersebut. \textit{View} pertama adalah pasangan teks sebelum dan sesudah revisi (bisa disebut juga sebagai pasangan T dan H) dan \textit{view} kedua adalah komentar penulis setelah melakukan revisi. 

Masukan untuk proses Co-training berupa data pasangan T dan H serta komentar penulis yang sebagian kecil telah dilabeli (bibit Co-training) dan selebihnya belum diberi label. Data tersebut diperoleh dari Wikipedia \textit{revision history} yang melalui beberapa tahap pengolahan, yaitu ekstraksi teks Wikipedia, pembentukan kandidat T dan H, anotasi manual, serta ekstraksi fitur. Dengan memasukkan sedikit data berlabel sebagai bibit, Co-training akan melabeli data tidak berlabel secara otomatis menggunakan dua \textit{classifier} yang bekerja terpisah pada masing-masing \textit{view}. Percobaan Co-training yang dilakukan pada penelitian ini, menunjukkan hasil yang cukup baik. Co-training hanya diberi bibit 400 data, namun dapat memperbesar ukuran data dengan menambahkan 1857 data baru. Akurasi dari hasil yang dikeluarkan juga cukup baik untuk ukuran penelitian pionir \textit{Textual Entailment} Bahasa Indonesia, yaitu 76\%. 

Data berlabel terakhir setelah Co-training berhenti dijadikan data isi korpus. Data tersebut merupakan pasangan kalimat pada artikel Wikipedia sebelum dan sesudah direvisi. Data yang dihasilkan cenderung mengarah ke \textit{Textual Entailment} tingkat leksikal karena perubahan yang terjadi hanya perbedaan penggunaan kata, seperti sinonim. Walaupun akurasi cukup baik, data yang dihasilkan cukup jenuh dan kurang bervariasi. Revisi yang terjadi umumnya adalah parafrase, sehingga nilai label \textit{entail} yang dihasilkan cukup mendominasi. Hal ini mungkin disebabkan karena revisi yang terjadi di Wikipedia didominasi dengan kasus yang seragam, contohnya revisi dengan bot mengubah kata dengan sinonimnya. Namun, jika dilihat dari segi ukuran, Wikipedia \textit{revision history} cukup memadai.

\Saya~berharap penelitian ini dapat menjadi motivasi untuk pengembangan \textit{Textual Entailment} Bahasa Indonesia selanjutnya. \textit{Textual Entailment} Bahasa Indonesia harus terus berkembang agar penelitian bidang NLP lain untuk Bahasa Indonesia dapat merasakan manfaat \textit{Textual Entailment}.

%-----------------------------------------------------------------------------%
\section{Saran}
%-----------------------------------------------------------------------------%
Setelah melakukan eksperimen dan menganalisis hasilnya, ada beberapa saran untuk penelitian selanjutnya, antara lain sebagai berikut.

\begin{enumerate}
	\item Co-training pada penelitian ini menggunakan salah satu jenis \textit{classifier} metode \textit{deep learning}, namun data untuk melatih \textit{classifier} tersebut data yang digunakan berukuran kecil, yaitu hanya 400 data. Sebaiknya, ukuran data berlabel sebagai bibit Co-training diperbesar. Hasil dari penelitian ini juga bisa digunakan kembali untuk memperbesar bibit pada penelitian selanjutnya.
	\item Pada penelitian ini, beberapa parameter pada saat menjalankan proses Co-training ditentukan secara heuristik tanpa melakukan percobaan, seperti batasan tingkat kepercayaan \textit{classifier} dalam melabeli data untuk menentukan apakah data berlabel tersebut baik, perbandingan jumlah data yang seimbang antara label E dan NE, serta cara pemangkasannya. Oleh karena itu, diharapkan penelitian selanjutnya melakukan percobaan terhadap penentuan parameter tersebut.
	\item Metode Co-training yang digunakan dapat dicoba dengan menggunakan kombinasi \textit{classifier} selain RNN atau Multinomial Naive Bayes. 
	\item Arsitektur RNN yang digunakan bisa lebih dikembangkan, misalnya dengan menambahkan lebih banyak fitur tambahan yang lebih menggambarkan hubungan T dan H atau desain arsitektur RNN baru yang lebih baik dan menentukan jumlah \textit{epoch} melalui proses percobaan.
	\item Amati lebih dalam mengenai fitur-fitur yang berpotensi memberikan informasi \textit{entailment} dari view komentar penulis. Pada penelitian ini \textit{view} komentar baru hanya menggunakan fitur-fitur yang sederhana. Tambahkan lagi fitur yang lebih relevan, misalnya menggunakan POS-Tag.
	\item Menjadikan nama akun penulis (kolaborator Wikipedia) sebagai salah satu pertimbangan dalam klasifikasi. Hal ini disarankan atas dasar adanya kemungkinan seorang penulis yang sama melakukan revisi pada beberapa artikel atau penulis yang sama lainnya kerap melakukan tindakan vandalisme. Permasalahan ini belum dipertimbangkan pada penelitian ini.
	\item Gunakan atau tambahkan sumber data lain selain Wikipedia Revision History. Wikipedia Revision History lebih cocok digunakan untuk RTE tingkat leksikal. 
	\item Jika menggunakan evaluasi \textit{sampling}, baiknya evaluasi dilakukan dalam beberapa kali. Evaluasi pada penelitian ini hanya sempat dilakukan sekali karena keterbatasan waktu. Sebaiknya evaluasi jenis \textit{sampling} dilakukan berkali-kali hingga akurasi konvergen.
\end{enumerate}